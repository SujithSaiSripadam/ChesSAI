# â™Ÿï¸ ChesSAI: A Hybrid Reinforcement Learning-Based Self-Evolving Chess Engine

A chess engine combining **PPO** and **MCTS**, trained via self-play, imitation learning, and dynamic ELO-based curriculum.

---

## ğŸš€ Overview

This repository implements a self-evolving chess AI that blends **Proximal Policy Optimization (PPO)** and **Monte Carlo Tree Search (MCTS)** into a unified hybrid architecture. Inspired by AlphaZero, our system introduces:

- Progressive training
- Curriculum-based self-play
- Reward shaping
- Dynamic policy mixing based on ELO

---

## ğŸ§  Core Components

- âœ… Supervised Pretraining using expert human games  
- â™»ï¸ Self-Play Reinforcement Learning with MCTS-guided policy improvement  
- ğŸ¤– Adversarial PPO Training against Stockfish  
- ğŸ” **Hybrid Action Selection**:  
  \[
  \pi_{\text{play}} = \alpha(\text{ELO}) \cdot \pi_{\text{PPO}} + (1 - \alpha(\text{ELO})) \cdot \pi_{\text{MCTS}}
  \]
- ğŸŒ² MCTS with Neural Guidance using UCB + policy priors  
- ğŸ§­ Reward Shaping based on positional central control  

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data/                # Preprocessed, self-play, and backup data
â”œâ”€â”€ models/              # Saved checkpoints across all training phases
â”œâ”€â”€ src/                 # All training, model, and MCTS source code
â”œâ”€â”€ play_vs_model.py     # GUI for human vs model play
â”œâ”€â”€ modelvsstockfish.py  # GUI for Stockfish vs model play
â”œâ”€â”€ environment.yaml     # Conda environment file
â”œâ”€â”€ requirements.txt     # Python requirements
â”œâ”€â”€ Chessai.pdf          # Detailed LaTeX report
```

---

## ğŸ—ï¸ Installation

1. **Clone the repository**  
```bash
git clone https://github.com/SujithSaiSripadam/ChesSAI.git
cd chessai
```

2. **Set up environment (Conda recommended)**  
```bash
conda env create -f environment.yaml
conda activate chessai
```

3. **Install Stockfish Engine**  
Download from: https://stockfishchess.org/download/

For MAC ğŸ users:

```bash
brew install stockfish
```

Add it to path
```bash
mv path/to/stockfish /usr/local/bin/
chmod +x /usr/local/bin/stockfish
```

For Linux ğŸ§ Users:
```bash
sudo apt install stockfish
```

To check stockfish:
```bash
which stockfish
```

---

## ğŸ§ª Running the Code

### âœ… Supervised Pretraining (Phase 1)
```bash
python -m src.train --phase 1
```

### â™»ï¸ Self-Play Training (Phase 2)
```bash
python -m src.train --phase 2
```

### ğŸ¤– PPO vs Stockfish Training (Phase 2.5)
```bash
python -m src.train_with_stockfish_V2
```

---

## ğŸ“Š Evaluation and Logging

TensorBoard logs are saved under:
```
./runs/PPO_...
```

To view logs:
```bash
tensorboard --logdir ./runs
```

---

## ğŸ“š Algorithms Explained

Full technical explanation is available in [`Chessai.pdf`](Chessai.pdf), covering:

- PPO loss derivation  
- MCTS pseudocode  
- Network architecture  
- Phase-wise training pipeline  

---

## ğŸ‘¨â€ğŸ’» Author

**Sripadam Sujith Sai**  
[GitHub]((https://github.com/SujithSaiSripadam/ChesSAI)) â€¢ [LinkedIn](https://www.linkedin.com/in/sripadam-sujith-sai/)   
_Evolving AI with algorithms and chess._

---

## ğŸ“ƒ License

This project is open-sourced under the MIT License.
